{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\jeffe/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\jeffe\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jeffe\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jeffe\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1d7934d914c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mfrase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mpalavras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mpalavras\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0menglish_stop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mpalavras_filtradas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpalavras\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0menglish_stop_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m     return [token for sent in sentences\n\u001b[0;32m    132\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\jeffe/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\jeffe\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jeffe\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jeffe\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan 19 09:49:17 2018\n",
    "\n",
    "@author: vitor\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#print('x' in np.arange(5))\n",
    "#leitura\n",
    "text = pd.read_csv('chennai_reviews.csv', sep=',',skip_blank_lines=True)\n",
    "train, test = train_test_split(text, test_size=0.2)#Usando 80% do data base\n",
    "vetor_atributo=[]#Vetor contendo as palavras usadas\n",
    "matrix=pd.Series.as_matrix(train)\n",
    "#Utilizo todos os adjetivos presentes no train.Review_Text segundo o nltk\n",
    "\n",
    "vetor_frases=[]\n",
    "for i in range(len(matrix)):#Criando o vetor com todas as frases\n",
    "    auxx=''\n",
    "    for j in range(len(matrix[i])):\n",
    "        if(isinstance(matrix[i][j], str)):\n",
    "            if(len(matrix[i][j])> len(auxx)):\n",
    "                auxx=matrix[i][j]\n",
    "    auxx=re.sub(r\"[^\\w\\s]\", '', auxx)#retirando os underscors\n",
    "    auxx=auxx.lower()\n",
    "    vetor_frases=np.append(vetor_frases,auxx)\n",
    "\n",
    "#Vetor de palavras\n",
    "for i in range(len(vetor_frases)):#Criando o vetor com atributos\n",
    "    frase=vetor_frases[i]\n",
    "    if isinstance(frase, str):\n",
    "        frase= re.sub(r\"[^\\w\\s]\", '', frase)#retirando os underscors\n",
    "        frase=frase.lower()\n",
    "        palavras = set(frase.split())\n",
    "        palavras=set(nltk.word_tokenize(frase))\n",
    "        english_stop_words = set(stopwords.words('english'))\n",
    "        palavras_filtradas = palavras - english_stop_words\n",
    "        tagged = nltk.pos_tag(palavras_filtradas)\n",
    "        aux=np.asmatrix(tagged)\n",
    "        aux1=np.where(aux=='JJ')#Pegando os adjetivos\n",
    "        if (len(np.where(aux=='JJ')[0])>0):\n",
    "            if not(tagged[aux1[0][0]][aux1[1][0]-1] in vetor_atributo):\n",
    "                vetor_atributo=np.append(vetor_atributo,tagged[aux1[0][0]][aux1[1][0]-1])\n",
    "#        aux1=np.where(aux=='RBS')\n",
    "#        if (np.sum(aux1)>0):\n",
    "#            if not(tagged[aux1[0][0]][aux1[1][0]-1] in vetor_atributo):\n",
    "#                vetor_atributo=np.append(vetor_atributo,tagged[aux1[0][0]][aux1[1][0]-1])\n",
    "#        aux1=np.where(aux=='JJS')\n",
    "#        if (np.sum(aux1)>0): \n",
    "#            if not(tagged[aux1[0][0]][aux1[1][0]-1] in vetor_atributo):\n",
    "#                vetor_atributo=np.append(vetor_atributo,tagged[aux1[0][0]][aux1[1][0]-1])\n",
    "#            \n",
    "\n",
    "#Valor de cada uma das porcentages dos sentimentos\n",
    "p1=0#porcentagem de 1\n",
    "p2=0#porcentagem de 2\n",
    "p3=0#porcentagem de 3\n",
    "error=[]#Vetor que irá conter os indices na matrix de alguma frase que gere error\n",
    "#for i in range(3813):\n",
    "#   if(matrix[i][3]=='1' or matrix[i][4]=='1'):\n",
    "#       p1=p1+1\n",
    "#   elif(matrix[i][3]=='2' or matrix[i][4]=='2'):\n",
    "#       p2=p2+1\n",
    "#   elif(matrix[i][3]=='3'or matrix[i][4]=='3'):\n",
    "#       p3=p3+1\n",
    "#   else:\n",
    "#       error=np.append(error,i)\n",
    "##for i in range(len(error)-1):\n",
    "##   if(len(np.where(matrix[int(error[0])]==1)[0])>0):\n",
    "##       p1=p1+1\n",
    "##   elif(len(np.where(matrix[int(error[0])]==2)[0])>0):\n",
    "##       p2=p2+1\n",
    "##   elif(len(np.where(matrix[int(error[0])]==3)[0])>0):\n",
    "##       p3=p3+1\n",
    "v=[]#Vetor com os setimentos\n",
    "for i in range(len(matrix)):#Criando o vetor com os sentimentos de cada frase\n",
    "   if(len(np.where(matrix[i]=='1')[0])>0 or len(np.where(matrix[i]==1)[0])>0):\n",
    "       v=np.append(v,int(1))\n",
    "   elif(len(np.where(matrix[i]=='2')[0])>0 or len(np.where(matrix[i]==2)[0])>0):\n",
    "       v=np.append(v,int(2))\n",
    "   elif(len(np.where(matrix[i]=='3')[0])>0 or len(np.where(matrix[i]==3)[0])>0):\n",
    "       v=np.append(v,int(3))\n",
    "   else:\n",
    "       error=np.append(error,i)\n",
    "       \n",
    "p1=len(np.where(v==1)[0])\n",
    "p2=len(np.where(v==2)[0])\n",
    "p3=len(np.where(v==3)[0])\n",
    "aux=[p1,p2,p3]\n",
    "p1=p1/np.sum(aux)\n",
    "p2=p2/np.sum(aux)\n",
    "p3=p3/np.sum(aux)\n",
    "aux=[p1,p2,p3]#Vetor probabilidade do sentimentos\n",
    "\n",
    "#matriz binária\n",
    "#vetor_frases=np.asarray(train.Review_Text)\n",
    "vetor_frases=[]\n",
    "for i in range(len(matrix)):#Criando o vetor com todas as frases\n",
    "    auxx=''\n",
    "    for j in range(len(matrix[i])):\n",
    "        if(isinstance(matrix[i][j], str)):\n",
    "            if(len(matrix[i][j])> len(auxx)):\n",
    "                auxx=matrix[i][j]\n",
    "    auxx=re.sub(r\"[^\\w\\s]\", '', auxx)\n",
    "    auxx=auxx.lower()\n",
    "    vetor_frases=np.append(vetor_frases,auxx)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "matriz_binaria=[]\n",
    "\n",
    "for i in range(len(vetor_frases)):#Criando uma matriz com vetores binarios em relação as\n",
    "    vetor_aux=[]\n",
    "    if isinstance(vetor_frases[i], str) and not(len(np.where(error==i)[0])) :    \n",
    "        for j in range(len(vetor_atributo)):\n",
    "            if(vetor_atributo[j] in vetor_frases[i].lower()):\n",
    "                vetor_aux=np.append(vetor_aux,int(1))\n",
    "            else:\n",
    "                vetor_aux=np.append(vetor_aux,int(0))\n",
    "        if(i!=0):\n",
    "            matriz_binaria=np.vstack([matriz_binaria,vetor_aux])\n",
    "        else:\n",
    "            matriz_binaria=vetor_aux\n",
    "    else:\n",
    "        error=np.append(error,i)\n",
    "    \n",
    "#Learning\n",
    "vetor_resultado=np.zeros([3,len(vetor_atributo)])\n",
    "maux=matriz_binaria.transpose()\n",
    "\n",
    "for i in range(len(maux)):\n",
    "    aux31=1#Quandtidade de vezes que o atributo[i]=1 aparece quando o valor do sentimento=3\n",
    "    aux11=1#Quandtidade de vezes que o atributo[i]=1 aparece quando o valor do sentimento=1\n",
    "    aux21=1#Quandtidade de vezes que o atributo[i]=1 aparece quando o valor do sentimento=2\n",
    "\n",
    "    perror=0#algumas frases podem não ser analisadas então é necessario ter a quantidade de erros acumulativos em cada frase\n",
    "#    for j in range(len(maux[0])):\n",
    "#        if(j in error):\n",
    "#            perror=perror+1\n",
    "#        if(v[j+perror]==1 and maux[i][j]==1 and not(j in error)):\n",
    "#            aux1=aux11+1\n",
    "#        elif(v[j+perror]==2 and maux[i][j]==1 and not(j in error)):\n",
    "#            aux2=aux21+1\n",
    "#        elif(v[j+perror]==3 and maux[i][j]==1 and not(j in error)):\n",
    "#            aux3=aux31+1  \n",
    "    \n",
    "    for j in range(len(maux[0])):\n",
    "        if(j in error):\n",
    "            perror=perror+1\n",
    "        if(v[j+perror]==1 and maux[i][j]==1 and not(j in error)):\n",
    "            aux11=aux11+1\n",
    "        if(v[j+perror]==2 and maux[i][j]==1 and not(j in error)):\n",
    "            aux21=aux21+1\n",
    "        if(v[j+perror]==3 and maux[i][j]==1 and not(j in error)):\n",
    "            aux31=aux31+1\n",
    "\n",
    "#    aux10=aux10/len(np.where(maux[i]==0)[0])\n",
    "#    aux20=aux20/len(np.where(maux[i]==0)[0])\n",
    "#    aux30=aux10/len(np.where(maux[i]==0)[0])\n",
    "#    aux11=aux10/len(np.where(maux[i]==1)[0])\n",
    "#    aux21=aux20/len(np.where(maux[i]==1)[0])\n",
    "#    aux31=aux10/len(np.where(maux[i]==1)[0])\n",
    "    vetor_resultado[0][i]=(aux11/(2+len(np.where(v==1)[0])))\n",
    "    vetor_resultado[1][i]=(aux21/(2+len(np.where(v==2)[0])))\n",
    "    vetor_resultado[2][i]=(aux31/(2+len(np.where(v==3)[0])))\n",
    "\n",
    "\n",
    " \n",
    "#test\n",
    "nmatrix=pd.Series.as_matrix(test)\n",
    "nvetor_frases=[]\n",
    "for i in range(len(nmatrix)):\n",
    "    auxx=''\n",
    "    for j in range(len(nmatrix[i])):\n",
    "        if(isinstance(nmatrix[i][j], str)):\n",
    "            if(len(nmatrix[i][j])> len(auxx)):\n",
    "                auxx=nmatrix[i][j]\n",
    "    auxx=re.sub(r\"[^\\w\\s]\", '', auxx)\n",
    "    auxx=auxx.lower()\n",
    "    nvetor_frases=np.append(nvetor_frases,auxx) \n",
    "nv=[]#Vetor de sentimentos do data test\n",
    "for i in range(len(nmatrix)):\n",
    "   if(len(np.where(nmatrix[i]=='1')[0])>0 or len(np.where(nmatrix[i]==1)[0])>0):\n",
    "       nv=np.append(nv,int(1))\n",
    "   elif(len(np.where(nmatrix[i]=='2')[0])>0 or len(np.where(nmatrix[i]==2)[0])>0):\n",
    "       nv=np.append(nv,int(2))\n",
    "   elif(len(np.where(nmatrix[i]=='3')[0])>0 or len(np.where(nmatrix[i]==3)[0])>0):\n",
    "       nv=np.append(nv,int(3))\n",
    "final=[]#Vetor binario dizendo se o algoritimo acertou\n",
    "final_result=[]#Resultado calculado de cada probabilidade\n",
    "fi=[]\n",
    "for z in range(len(nvetor_frases)):\n",
    "    string_test=nvetor_frases[z]#Texto para teste\n",
    "    vetor_str_test=[]#Vetor auxiliar binario da frase em relação aos atributos\n",
    "    result=0\n",
    "    test_result=np.zeros([3,1])\n",
    "       \n",
    "    for j in range(len(vetor_atributo)):#Criando o vetor_str_test\n",
    "        if(vetor_atributo[j] in nvetor_frases[i].lower()):\n",
    "            vetor_str_test=np.append(vetor_str_test,int(1))\n",
    "        else:\n",
    "            vetor_str_test=np.append(vetor_str_test,int(0))\n",
    "    \n",
    "    for i in range(len(vetor_resultado)):#Fazendo o calculo da probabilidade de ser cada um dos sentimentos\n",
    "        result=math.log10(aux[i])\n",
    "        for j in range(len(vetor_resultado[i])):\n",
    "            result=result+math.log10(vetor_str_test[j]*(vetor_resultado[i][j])+(1-vetor_str_test[j])*((1-vetor_resultado[i][j])))#]Modelo de Bernoulli\n",
    "        test_result[i][0]=result\n",
    "        \n",
    "    aux4=np.where(np.max(test_result)==test_result)[0]==(nv[z]-1)#Teste para ver se o valor encontrado pelo algoritimo é o mesmo do data test\n",
    "    final=np.append(final,aux4[0])\n",
    "    fi=np.append(fi,z)\n",
    "    final_result=np.append(final_result,test_result)\n",
    "    \n",
    "raiting=len(np.where(final==True)[0])/len(final)#Taxa de acerto   \n",
    "final_result=np.reshape(final_result,[3,len(nvetor_frases)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
